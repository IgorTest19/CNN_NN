{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CNN -> NN - werjsa alfa do testowania poprawności działania sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Instalacja pomocniczych, pythonowskich bibliotek obliczeniowych.\"\"\"\n",
    "! pip3 install numpy\n",
    "! pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Instalacja narzędzi pomocniczych do modyfikacji kodu.\"\"\"\n",
    "! pip3 install - -upgrade autopep8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Instalacja zbioru danych EMNIST, reprezentującego znaki alfanumeryczne\"\"\"\n",
    "! pip3 install emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emnist import extract_test_samples\n",
    "from emnist import extract_training_samples\n",
    "from emnist import list_datasets\n",
    "\n",
    "\"\"\"Importowanie podzbioru typu balanced z EMNIST.\"\"\"\n",
    "list_datasets()\n",
    "\n",
    "\"\"\"Przypisanie danych treningowych do poszczególnych zmiennych\"\"\"\n",
    "train_images, train_labels = extract_training_samples(\n",
    "    'balanced')  # przypisanie danych do poszczególnych zmiennych\n",
    "print(\"Wymiary danych treningowych:\", train_images.shape)\n",
    "print(\"Wymiary etykiet od danych treningowych:\", train_labels.shape)\n",
    "\n",
    "\"\"\"Przypisanie danych testowych do poszczególnych zmiennych\"\"\"\n",
    "test_images, test_labels = extract_test_samples('balanced')\n",
    "print(\"\\nWymiary danych testowych:\", test_images.shape)\n",
    "print(\"Wymiary etykiet od danych testowych:\", test_labels.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wczytywanie wcześniej importowanych bibliotek.\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wyświetlenie pierwszych dziesięciu elementów zbioru treningowego.\"\"\"\n",
    "for x in range(10):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    imgplot = plt.imshow(train_images[x], cmap='gray')\n",
    "    ax.set_title(\"Image label: \" + str(train_labels[x]))\n",
    "    plt.colorbar(ticks=[0, 50, 100, 150, 200, 255], orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wyświetlenie pierwszych dziesięciu elementów zbioru testowego.\"\"\"\n",
    "for x in range(10):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    imgplot = plt.imshow(test_images[x], cmap='gray')\n",
    "    ax.set_title(\"Image label: \" + str(test_labels[x]))\n",
    "    plt.colorbar(ticks=[0, 50, 100, 150, 200, 255], orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1000):\n",
    "    if test_labels[x] == 44:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "        imgplot = plt.imshow(test_images[x], cmap='gray')\n",
    "        ax.set_title(\"Image label: \" + str(test_labels[x]))\n",
    "        plt.colorbar(ticks=[0, 50, 100, 150, 200, 255], orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wyświetlenie etykiet zbioru uczącego oraz testowego.\"\"\"\n",
    "print(\"First 20 training set images labels:\")\n",
    "# nie musimy robić shuffle, bo wartości są już pomieszane. Czasem dane są posegregowane.\n",
    "print(train_labels[:20])\n",
    "print(\"\\nFirst 20 testing images labels:\")\n",
    "print(test_labels[:20])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    '''\n",
    "    Funkcja dokonująca zmiany wartości na zakres od 0 do 1 dla zakresu pikseli od 0 do 255.\n",
    "\n",
    "    Args:\n",
    "        x(int/arr): wartość poddawana normalizacji do zakresu od 0 do 1.\n",
    "\n",
    "    Returns:\n",
    "        Znormalizowana wartość x.\n",
    "    '''\n",
    "\n",
    "    return x/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with(vector, pad_width, iaxis, kwargs):\n",
    "    '''\n",
    "    Funkcja dodające obramowanie do macierzy wejśćiowej.\n",
    "\n",
    "    Args:\n",
    "        vector(arr): Tablica poddawana modyfikacji.\n",
    "        pad_with(arr): Wartość obramowania.\n",
    "        iaxis(arr): Oś obramowania.\n",
    "    '''\n",
    "\n",
    "    pad_value = kwargs.get('padder', 0)\n",
    "    vector[:pad_width[0]] = pad_value\n",
    "    vector[-pad_width[1]:] = pad_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Funkcja aktywacji sigmoid.\n",
    "\n",
    "    Args:\n",
    "        x(arr/int): przekazywana wartość do funkcji.\n",
    "\n",
    "    Returns:\n",
    "        (float): Wartość funkcji sigmoid. Zakres liczbowy (0:1)\n",
    "\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    '''\n",
    "    Funkcja aktywacji ReLu.\n",
    "\n",
    "    Args:\n",
    "        x(arr/int): przekazywana wartość do funkcji.\n",
    "\n",
    "    Returns:\n",
    "        (int/float):Wartość funkcji ReLu. Zakres liczbowy (0:inf)\n",
    "\n",
    "    '''\n",
    "    return np.maximum(0.0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa reprezentująca warstwę konwolucyjną - wersja nr 2 dla wielu filtrów 3d\n",
    "\n",
    "class ConvolutionalLayer:\n",
    "    \"\"\"Klasa reprezentująca warstwę konwolucyjną, jej paramtery, generowanie filtrów, operację konwolucji.\"\"\"\n",
    "\n",
    "    def __init__(self, filters_num, filter_h, filter_w, filter_d, stride, cnn_layer_no, padding):\n",
    "        \"\"\"Konstruktor klasy.\n",
    "\n",
    "        Args:\n",
    "            filters_num(int): Ilość filtrów.\n",
    "            filter_h(int): Wysokość filtra.\n",
    "            filter_w(int): Szerokość filtra.\n",
    "            filter_d(int): Głębokość filtra.\n",
    "            stride(int): Krok w operacji konwolucji\n",
    "\n",
    "        \"\"\"\n",
    "        self.filters_num = filters_num\n",
    "        self.stride = stride\n",
    "        self.filter_h = filter_h\n",
    "        self.filter_w = filter_w\n",
    "        self.filter_d = filter_d\n",
    "        self.cnn_layer_no = cnn_layer_no\n",
    "        self.padding = padding.capitalize()\n",
    "\n",
    "        \"\"\"Losowe filtry z zakresu od 0 do 1\"\"\"\n",
    "\n",
    "        \"\"\"najszybsza i najładniejsza opcja\"\"\"\n",
    "        self.filters = np.array([np.random.rand(\n",
    "            self.filter_d, self.filter_h, self.filter_w) - 0.5 for x in range(self.filters_num)])\n",
    "\n",
    "        \"\"\"Wygenerowanie biasów.\"\"\"\n",
    "        self.biases = np.ones(self.filters_num)\n",
    "\n",
    "    def conv_process(self, input):\n",
    "        \"\"\"Operacja konwolucji/splotu na zadanym obrazie.\n",
    "\n",
    "        Args:\n",
    "            input(numpy array): Obraz wejściowy.\n",
    "\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "\n",
    "        \"\"\"Odczytanie wymiarów obrazu w zależnoścci od ilości kanałów\"\"\"\n",
    "        if len(self.input.shape) == 2:\n",
    "            self.height, self.width = self.input.shape  # wymiary zwykłego obrazu 2d\n",
    "        else:\n",
    "            self.depth, self.height, self.width = self.input.shape  # wymiary zwykłego obrazu 3d\n",
    "\n",
    "        \"\"\"Sprawdzenie czy głębokość filtra odpowiada głębokości kanału.\"\"\"\n",
    "        if (len(self.input.shape) == 3 and self.depth == self.filter_d) or len(self.input.shape) == 2:\n",
    "\n",
    "            \"\"\"Dodanie paddingu do danych wejściowych.\"\"\"\n",
    "            if self.padding == \"True\":\n",
    "                self.image_padded = np.pad(self.input, 1, pad_with)\n",
    "                if len(self.input.shape) == 3:\n",
    "                    # wycinamy kanały pierwszy i ostatni złożone z samych zer\n",
    "                    self.image_padded = self.image_padded[1:-1]\n",
    "            else:\n",
    "                self.image_padded = self.input\n",
    "\n",
    "            \"\"\"Odczytanie wymiarów obrazu z paddingiem w zależnoścci od ilości kanałów.\"\"\"\n",
    "            if len(self.input.shape) == 2:\n",
    "                # wymiary obrazu z paddingiem\n",
    "                self.height_pad, self.width_pad = self.image_padded.shape\n",
    "            else:\n",
    "                # wymiary obrazu 3d z paddinguem\n",
    "                self.height_pad, self.width_pad = self.image_padded.shape[1:]\n",
    "\n",
    "            \"\"\"Końcowa granica pętli dokonującej operacji konwolucji/splotu.\"\"\"\n",
    "            self.stride_range = int(\n",
    "                ((self.width - self.filter_w + 2 * 1) / self.stride) + 1)\n",
    "\n",
    "            \"\"\"Wymiary macierzy wyjściowej.\"\"\"\n",
    "            self.shape_dim = int(\n",
    "                ((self.width - self.filter_w + 2 * 1) / self.stride) + 1)\n",
    "\n",
    "            \"\"\"tablica pomocnicza dla wyników.\"\"\"\n",
    "            self.conv_output = np.zeros(\n",
    "                (self.filters_num, self.shape_dim, self.shape_dim))\n",
    "\n",
    "            \"\"\"Proces konwolucji.\"\"\"\n",
    "            for f in range(self.filters_num):\n",
    "                y = 0\n",
    "                for h in range(0, self.stride_range, self.stride):\n",
    "                    x = 0\n",
    "                    for w in range(0, self.stride_range, self.stride):\n",
    "                        \"\"\"Wyznaczenie przestrzeni danych wejściowych 3d lub obszaru 2d do konwolucji.\"\"\"\n",
    "                        if len(self.input.shape) == 3:\n",
    "\n",
    "                            # [depth, height, width]\n",
    "                            self.temp_arr = self.image_padded[0:self.depth,\n",
    "                                                              h:h + self.filter_h, w:w + self.filter_w]\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            self.temp_arr = self.image_padded[h:h +\n",
    "                                                              self.filter_h, w:w + self.filter_w]\n",
    "                        self.conv = (np.multiply(\n",
    "                            self.filters[f], self.temp_arr)).mean() + self.biases[f]\n",
    "                        self.conv_output[f][y][x] = self.conv\n",
    "                        x += 1\n",
    "                    y += 1\n",
    "\n",
    "            self.conv_output = np.where(\n",
    "                self.conv_output > 255, 255, self.conv_output)\n",
    "\n",
    "        else:\n",
    "            print(\"Głębokość filtra różni się od głębokości danych wejśćiowych. Proces kowolucji nie został wykonany.\")\n",
    "\n",
    "    def backwards(self, dvalues):\n",
    "        \"\"\"Funkcja dokonująca wstecznej propagacji warstwy konwolucyjnej.\n",
    "\n",
    "        Args:\n",
    "              dvalues(numpy array): Obraz wejściowy ze wstecznej propagacji.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dvalues = dvalues\n",
    "        self.dv_filters_180 = np.copy(self.filters)\n",
    "\n",
    "        # obrócenie filtrów o 180 stopni względem osi X i Y\n",
    "        for i in range(self.filters_num):\n",
    "            for d in range(self.filter_d):\n",
    "                self.dv_filters_180[i][d] = np.rot90(\n",
    "                    self.dv_filters_180[i][d], 2, axes=(0, 1))\n",
    "        # Transponowanie filtrów w celu zgodności wymiarów dla wstecznej propagacji\n",
    "        # Zmiana miejscami osi: ilość filtrów, głębokość\n",
    "        self.dv_filters_T = np.moveaxis(self.dv_filters_180, 0, 1)\n",
    "        self.dv_filters_num, self.dv_filters_d, self.dv_filters_h, self.dv_filters_w = self.dv_filters_T.shape\n",
    "        \"\"\"Odczytanie wymiarów obrazu w zależnoścci od ilości kanałów\"\"\"\n",
    "        if len(self.dvalues.shape) == 2:\n",
    "            self.dv_height, self.dv_width = self.dvalues.shape  # wymiary zwykłego obrazu 2d\n",
    "        else:\n",
    "            # wymiary zwykłego obrazu 3d\n",
    "            self.dv_depth, self.dv_height, self.dv_width = self.dvalues.shape\n",
    "\n",
    "        \"\"\"padding\"\"\"\n",
    "        if len(self.dvalues.shape) == 2:\n",
    "            self.dvalues_padded = np.pad(self.dvalues, 1, pad_with)\n",
    "        else:\n",
    "            self.dvalues_padded = np.pad(self.dvalues, 1, pad_with)\n",
    "            # wycinamy kanały pierwszy i ostatni złożone z samych zer\n",
    "            self.dvalues_padded = self.dvalues_padded[1:-1]\n",
    "\n",
    "        \"\"\"Odczytanie wymiarów obrazu z paddingiem w zależnoścci od ilości kanałów.\"\"\"\n",
    "        if len(self.dvalues.shape) == 2:\n",
    "            # wymiary obrazu z paddingiem\n",
    "            self.dv_height_pad, self.dv_width_pad = self.dvalues_padded.shape\n",
    "        else:\n",
    "            # wymiary obrazu 3d z paddinguem\n",
    "            self.dv_height_pad, self.dv_width_pad = self.dvalues_padded.shape[1:]\n",
    "\n",
    "        \"\"\"Końcowa granica pętli dokonującej operacji konwolucji/splotu.\"\"\"\n",
    "#         self.dv_stride_range = int(self.dv_width_pad - np.ceil(self.filter_w/2)) # dla obrazu o wymiarze 28x28 i 30x30(z paddingiem) stride_range = 30\n",
    "        self.dv_stride_range = int(\n",
    "            ((self.dv_width - self.filter_w + 2 * 1) / self.stride) + 1)\n",
    "\n",
    "        \"\"\"Wymiary macierzy wyjściowej.\"\"\"\n",
    "        self.dv_shape_dim = int(\n",
    "            ((self.dv_width - self.filter_w + 2 * 1) / self.stride) + 1)\n",
    "\n",
    "        \"\"\"tablica pomocnicza dla wyników.\"\"\"\n",
    "        self.conv_dvalues = np.zeros(\n",
    "            (self.dv_filters_num, self.dv_shape_dim, self.dv_shape_dim))\n",
    "\n",
    "        if self.cnn_layer_no != 1:\n",
    "            \"\"\"Proces konwolucji.\"\"\"\n",
    "            for f in range(self.dv_filters_num):\n",
    "                y = 0\n",
    "                for h in range(0, self.dv_stride_range, self.stride):\n",
    "                    x = 0\n",
    "                    for w in range(0, self.dv_stride_range, self.stride):\n",
    "                        \"\"\"Wyznaczenie przestrzeni danych wejściowych 3d lub obszaru 2d do konwolucji.\"\"\"\n",
    "                        if len(self.dvalues.shape) == 3:\n",
    "                            # [depth, height, width]\n",
    "                            self.dv_temp_arr = self.dvalues_padded[0:self.dv_depth,\n",
    "                                                                   h:h + self.dv_filters_h, w:w + self.dv_filters_w]\n",
    "                        else:\n",
    "                            self.dv_temp_arr = self.dvalues_padded[h:h +\n",
    "                                                                   self.dv_filters_h, w:w + self.dv_filters_w]\n",
    "\n",
    "                        self.dv_conv = (np.multiply(\n",
    "                            self.dv_filters_T[f], self.dv_temp_arr)).mean()\n",
    "\n",
    "                        self.conv_dvalues[f][y][x] = self.dv_conv\n",
    "\n",
    "                        x += 1\n",
    "                    y += 1\n",
    "\n",
    "        \"\"\"WSTECZNA PROPAGACJA DLA FILTRÓW\"\"\"\n",
    "        # filtry\n",
    "\n",
    "        \"\"\"Wsteczna propagacjia dla filtrów v4.\"\"\"\n",
    "\n",
    "        if len(self.image_padded.shape) == 2:\n",
    "            self.image_padded_backprop = self.image_padded[np.newaxis, :, :]\n",
    "        else:\n",
    "            self.image_padded_backprop = self.image_padded\n",
    "\n",
    "        if len(self.dvalues) == 2:\n",
    "            self.dvalues_backprop = self.dvalues[np.newaxis, :, :]\n",
    "        else:\n",
    "            self.dvalues_backprop = self.dvalues\n",
    "\n",
    "        \"\"\"v1\"\"\"\n",
    "        self.dw_filters = np.zeros(self.filters.shape)\n",
    "        for f in range(self.filters_num):\n",
    "            divider = np.count_nonzero(self.dvalues_backprop[f] > 0)\n",
    "            for d in range(self.filter_d):\n",
    "                y = 0\n",
    "                for h in range(0, self.dv_stride_range, self.stride):\n",
    "                    x = 0\n",
    "                    for w in range(0, self.dv_stride_range, self.stride):\n",
    "                        self.dw_filters[f][d] += (self.image_padded_backprop[d, h:h + self.dv_filters_h, w:w + self.dv_filters_w])*(\n",
    "                            self.dv_filters_w * self.dv_filters_w) * self.dvalues_backprop[f][y][x]\n",
    "                        x += 1\n",
    "                    y += 1\n",
    "\n",
    "        self.dv_biases = np.zeros(self.biases.shape)\n",
    "        for n in range(self.filters_num):\n",
    "            self.dv_biases[n] = np.sum(self.dvalues_backprop[n])\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.biases = self.biases - learning_rate * self.dv_biases\n",
    "        self.filters = self.filters - learning_rate * self.dw_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min, f_max = np.amin(img1), np.amax(img1)\n",
    "img1 = (img1 - f_min) / (f_max - f_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = ConvolutionalLayer(1, 5, 5, 1, 1, 2, \"true\")\n",
    "print(c1.filters)\n",
    "c1.conv_process(train_images[0])\n",
    "print(c1.conv_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c1.conv_output.shape)\n",
    "for x in c1.conv_output:\n",
    "    img1 = x\n",
    "    f_min, f_max = img1.min(), img1.max()\n",
    "    img1 = (img1 - f_min) / (f_max - f_min)\n",
    "    plt.show(plt.imshow(img1, cmap='gray'))\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.backwards(c1.conv_output)\n",
    "print(c1.filters)\n",
    "print(c1.dv_filters_180)\n",
    "print(c1.conv_output)\n",
    "print(c1.conv_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa reprezentująca warstwę funkcji aktywacji ReLu - Rectified Linear Unit\n",
    "class ReLu:\n",
    "    def forwards(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.array([relu(x) for x in self.inputs])\n",
    "\n",
    "    def backwards(self, dvalues):\n",
    "\n",
    "        self.dvalues = dvalues\n",
    "\n",
    "        self.dvalues_reshaped = self.dvalues.reshape(self.inputs.shape)\n",
    "\n",
    "        self.dinputs = self.dvalues_reshaped.copy()\n",
    "\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wersja najnowsza z dnia 06-04-2021.\"\"\"\n",
    "# Klasa reprezentująca warstwę pooling\n",
    "\n",
    "\n",
    "class MaxPooling:\n",
    "    def __init__(self, pooling_dim, stride):\n",
    "        self.pooling_dim = pooling_dim\n",
    "        self.stride = stride\n",
    "\n",
    "    def forwards(self, input):\n",
    "        \"\"\"Funkcja przekazująca wartości w głąb sieci.\"\"\"\n",
    "        self.input = input\n",
    "\n",
    "        \"\"\"Odczytanie wymiarów wejścia w zależności od ilości kanałów.\"\"\"\n",
    "        if len(self.input.shape) == 2:\n",
    "            self.height, self.width = self.input.shape  # wymiary dla danych wejścowych 2d\n",
    "        else:\n",
    "            # wymiary dla danych wejściowych 3d\n",
    "            self.depth, self.height, self.width = self.input.shape\n",
    "\n",
    "        \"\"\"Obliczenie wymiarów wyjściowych w osi X/Y po operacji poolingu.\"\"\"\n",
    "        # W2 = (W1 - F) / S + 1\n",
    "        self.output_dim = int(\n",
    "            np.ceil((self.height - self.pooling_dim) / self.stride + 1))\n",
    "\n",
    "        \"\"\"Zdefiniowanie pierwotnej, pomocniczej macierzy(tensora) wyjściowej złożonej z zer.\"\"\"\n",
    "        if len(self.input.shape) == 2:\n",
    "            self.pooling_output = np.zeros((self.output_dim, self.output_dim))\n",
    "        else:\n",
    "            self.pooling_output = np.zeros(\n",
    "                (self.depth, self.output_dim, self.output_dim))\n",
    "\n",
    "        \"\"\"Tablica pomocnicza do zapisania współrzędnych max wartości poolingu.\"\"\"\n",
    "        self.pooling_coordinates = self.input.copy()\n",
    "\n",
    "        def pooling_ranges(iter_range, pooling_dim, input_dim):\n",
    "            \"\"\"Funkcja zwracająca wymiary zakresu poolingu, w przypadku,\n",
    "                gdy wykroczy on poza zakres macierzy wejściowej.\n",
    "\n",
    "            Args:\n",
    "                iter_range(int): Wymiar(wyokość/szerokość) macierzy wynikający z iteracji.\n",
    "                pooling_dim(int): Wymiar zakresu poolingu.\n",
    "                input_dim(int): Wymiar(wyokość/szerokość) macierzy wejśćiowej\n",
    "\n",
    "            Return:\n",
    "                start(int): Początek zakresu poolingu.\n",
    "                end(int): Koniec zakresu poolingu\n",
    "\n",
    "            \"\"\"\n",
    "            if iter_range + pooling_dim > input_dim:\n",
    "                start = input_dim - pooling_dim\n",
    "                end = input_dim\n",
    "            else:\n",
    "                start = iter_range\n",
    "                end = iter_range + pooling_dim\n",
    "            return start, end\n",
    "\n",
    "        \"\"\"Operacja poolingu odpowiednio dla jednego kanału oraz więcej niż jeden.\"\"\"\n",
    "        if len(self.input.shape) == 2:\n",
    "            y = 0  # wymiar po wysokości tablicy wyjściowej\n",
    "            for h in range(0, self.height, self.stride):\n",
    "                x = 0  # wymiar po szerokości tablicy wyjściowej\n",
    "\n",
    "                \"\"\"Sprawdzenie, czy zakres poolingu nie wychodzi poza wysokość macierzy.\"\"\"\n",
    "                self.height_start, self.height_end = pooling_ranges(\n",
    "                    h, self.pooling_dim, self.height)\n",
    "\n",
    "                for w in range(0, self.width, self.stride):\n",
    "\n",
    "                    \"\"\"Sprawdzenie, czy zakres poolingu nie wychodzi poza szerokość macierzy.\"\"\"\n",
    "                    self.width_start, self.width_end = pooling_ranges(\n",
    "                        w, self.pooling_dim, self.width)\n",
    "\n",
    "                    self.area = self.input[self.height_start:self.height_end,\n",
    "                                           self.width_start:self.width_end]\n",
    "                    self.pooling_output[y][x] = np.amax(self.area)\n",
    "\n",
    "                    if self.pooling_output[y][x] != 0:\n",
    "                        i_1, j_1 = np.where(\n",
    "                            self.area == self.pooling_output[y][x])\n",
    "                        if len(i_1) > 1 or len(j_1) > 1:\n",
    "                            i_1 = [i_1[0]]\n",
    "                            j_1 = [j_1[0]]\n",
    "                        i_1 = i_1[0]\n",
    "                        j_1 = j_1[0]\n",
    "                        self.temp = np.zeros((self.stride, self.stride))\n",
    "                        self.temp[i_1][j_1] = 1\n",
    "                        self.pooling_coordinates[self.height_start:self.height_end,\n",
    "                                                 self.width_start:self.width_end] = self.temp\n",
    "                        self.coordinates_arr = []\n",
    "                        for h_c in range(0, len(self.pooling_coordinates), 1):\n",
    "                            for w_c in range(0, len(self.pooling_coordinates[0]), 1):\n",
    "                                if self.pooling_coordinates[h_c][w_c] == 1:\n",
    "                                    self.coordinates_arr.append([h_c, w_c])\n",
    "\n",
    "                        \"\"\"TRZEBA POPRAWIĆ NA NORMALNY, czytelny KOD...\"\"\"\n",
    "\n",
    "                    x += 1\n",
    "                y += 1\n",
    "        else:\n",
    "            self.coordinates_arr = []\n",
    "            for d in range(0, self.depth):\n",
    "                y = 0  # wymiar po wysokości tablicy wyjściowej\n",
    "                # print(len(self.coordinates_arr))\n",
    "                for h in range(0, self.height, self.stride):\n",
    "                    x = 0  # wymiar po szerokości tablicy wyjściowej\n",
    "\n",
    "                    \"\"\"Sprawdzenie, czy zakres poolingu nie wychodzi poza wysokość macierzy.\"\"\"\n",
    "                    self.height_start, self.height_end = pooling_ranges(\n",
    "                        h, self.pooling_dim, self.height)\n",
    "\n",
    "                    for w in range(0, self.width, self.stride):\n",
    "\n",
    "                        \"\"\"Sprawdzenie, czy zakres poolingu nie wychodzi poza szerokość macierzy.\"\"\"\n",
    "                        self.width_start, self.width_end = pooling_ranges(\n",
    "                            w, self.pooling_dim, self.width)\n",
    "\n",
    "                        # print(self.input[d, self.height_start:self.height_end, self.width_start:self.width_end])\n",
    "                        self.area = self.input[d, self.height_start:self.height_end,\n",
    "                                               self.width_start:self.width_end]\n",
    "                        self.pooling_output[d][y][x] = np.amax(self.area)\n",
    "\n",
    "                        \"\"\"Wynaczenie występowania wszystkich max wartośći z max poolingu.\"\"\"\n",
    "                        i_1, j_1 = np.where(\n",
    "                            self.area == self.pooling_output[d][y][x])\n",
    "                        if len(i_1) > 1 or len(j_1) > 1:\n",
    "                            i_1 = [i_1[0]]\n",
    "                            j_1 = [j_1[0]]\n",
    "                        i_1 = i_1[0]\n",
    "                        j_1 = j_1[0]\n",
    "                        self.temp = np.zeros((self.stride, self.stride))\n",
    "                        self.temp[i_1][j_1] = 1\n",
    "                        self.pooling_coordinates[d, self.height_start:self.height_end,\n",
    "                                                 self.width_start:self.width_end] = self.temp\n",
    "\n",
    "                        x += 1\n",
    "                    y += 1\n",
    "\n",
    "            \"\"\"\"Przypisanie współrzędnych z max poolingu do tablicy.\"\"\"\n",
    "            for d_c in range(0, self.depth, 1):\n",
    "                for h_c in range(0, len(self.pooling_coordinates[0]), 1):\n",
    "                    for w_c in range(0, len(self.pooling_coordinates[0]), 1):\n",
    "                        if self.pooling_coordinates[d_c][h_c][w_c] == 1:\n",
    "                            self.coordinates_arr.append([d_c, h_c, w_c])\n",
    "\n",
    "    def backwards1(self, dvalues):\n",
    "        \"\"\"Funkcja wstecznej propagacji.\"\"\"\n",
    "        self.dvalues = dvalues\n",
    "        # algorytm dokonujący zmiany wymiarów na 2D / 3D w zależoności od danych wejściowych\n",
    "        if len(self.dvalues.shape) == 1:\n",
    "            self.reshaped_input = self.dvalues.reshape(\n",
    "                self.pooling_output.shape)\n",
    "        elif len(self.dvalues.shape) == 2:\n",
    "            self.dvalues_height, self.dvalues_width = self.dvalues.shape\n",
    "            if self.dvalues_height != self.dvalues_width:\n",
    "                self.reshaped_input = self.dvalues.reshape(\n",
    "                    self.pooling_output.shape)\n",
    "            else:\n",
    "                self.reshaped_input = self.dvalues\n",
    "        else:\n",
    "            self.reshaped_input = self.dvalues\n",
    "\n",
    "        \"\"\"Test max pooling jest dobrze zrobiony.\"\"\"\n",
    "        self.back_arr = np.zeros(self.input.shape)\n",
    "        for x in range(len(self.coordinates_arr)):\n",
    "            d, h, w = self.coordinates_arr[x]\n",
    "            self.back_arr[d][h][w] = 1\n",
    "\n",
    "        self.reshaped_depth, self.reshaped_height, self.reshaped_width = self.reshaped_input.shape\n",
    "        self.dinput = self.back_arr.copy()\n",
    "        i = 0\n",
    "        for d in range(self.depth):  # for d in range(self.reshaped_depth):\n",
    "            for h in range(self.height):\n",
    "                j = 0\n",
    "                for w in range(self.width):\n",
    "                    if self.dinput[d][h][w] == 1:\n",
    "                        self.dinput[d][h][w] = self.dinput[d][h][w] * \\\n",
    "                            (self.reshaped_input.flatten())[j]\n",
    "                        j = j + 1\n",
    "                i = i + 1\n",
    "\n",
    "    def backwards(self, dvalues):\n",
    "        \"\"\"Funkcja wstecznej propagacji.\"\"\"\n",
    "        self.dvalues = dvalues\n",
    "\n",
    "        if len((self.dvalues).shape) != 1:\n",
    "            self.dvalues = self.dvalues.flatten()\n",
    "\n",
    "        \"\"\"Test max pooling jest dobrze zrobiony.\"\"\"\n",
    "        self.back_arr = np.zeros(self.input.shape)\n",
    "        for x in range(len(self.coordinates_arr)):\n",
    "            d, h, w = self.coordinates_arr[x]\n",
    "            self.back_arr[d][h][w] = 1\n",
    "\n",
    "        self.dinput = self.back_arr.copy()\n",
    "\n",
    "        i = 0\n",
    "        for d in range(self.depth):\n",
    "            for h in range(self.height):\n",
    "                for w in range(self.width):\n",
    "                    if self.dinput[d][h][w] == 1:\n",
    "\n",
    "                        self.dinput[d][h][w] = self.dinput[d][h][w] * \\\n",
    "                            self.dvalues[i]\n",
    "                        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOBRA WERSJA, WSZYSTKO DZIAŁĄ ELEGANCKO\n",
    "class NeuralNetwork6:\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons, nn_layer_no):\n",
    "        self.nn_layer_no = nn_layer_no\n",
    "        \"\"\"He weight initialization.\"\"\"\n",
    "        self.weights = np.random.rand(n_inputs, n_neurons) - 0.5\n",
    "        self.biases = np.ones(n_neurons)\n",
    "\n",
    "    def forwards(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backwards(self, dvalues):\n",
    "        \"\"\"Funkcja dokonująca proces wstecznej propagacji błędu, wyznaczając zmianę/deltę wag oraz biasów.\n",
    "\n",
    "        Args:\n",
    "            dvalues(numpy array): wektor/macierz złożona z pochodnych, przekazanych wstecz, z warstwy następnej. \n",
    "        # \"\"\"\n",
    "        self.dvalues = dvalues\n",
    "        self.dweights = np.dot(\n",
    "            (self.inputs[np.newaxis, :]).T, self.dvalues[np.newaxis, :])  # wariant 2\n",
    "\n",
    "        if self.nn_layer_no != 1:\n",
    "            # wyznaczenie zmiennych idących dalej, podawanych do poprzedniej warstwy\n",
    "            self.dinputs = np.dot(self.dvalues, self.weights.T)\n",
    "\n",
    "        # biasy\n",
    "        # wyznaczenie zmian/delt dla biasów\n",
    "        self.dbiases = self.dvalues * np.ones(self.biases.shape)\n",
    "\n",
    "    # SGD s 247 NN - można stworzyć osobną klasę do updatowania wag.\n",
    "    def update(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.biases = self.biases - learning_rate * self.dbiases\n",
    "        self.weights = self.weights - learning_rate * self.dweights\n",
    "\n",
    "\n",
    "class Sigmoid6:\n",
    "\n",
    "    def forwards(self, inputs):\n",
    "        self.output = sigmoid(inputs)\n",
    "\n",
    "    def backwards(self, dvalues):\n",
    "        # dE/do1 * do1/dzo1 | dE/dout * dout/dnet\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "\n",
    "class Softmax6:\n",
    "\n",
    "    def forwards(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # zapobieganie wysokim wartościom exp / overflow\n",
    "        self.exp_inputs = np.exp(self.inputs - np.max(self.inputs))\n",
    "        self.exp_inputs_norm = self.exp_inputs / (np.sum(self.exp_inputs))\n",
    "\n",
    "    def backwards(self, dvalues):\n",
    "        self.dvalues = dvalues\n",
    "        self.dinputs = dvalues * \\\n",
    "            (self.exp_inputs_norm*(1-self.exp_inputs_norm))\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "\n",
    "    def forwards(self, inputs, target):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "        self.y_pred_clipped = np.clip(self.inputs, 1e-7, 1-1e-7)\n",
    "        self.y_pred = (-1)*(np.sum(np.log(self.y_pred_clipped) * self.target))\n",
    "        self.y_pred2 = -np.mean(np.log(self.y_pred_clipped) * self.target)\n",
    "\n",
    "    def backwards(self, dvalues, target):\n",
    "        self.dvalues = dvalues\n",
    "        self.target = target\n",
    "        samples = len(self.dvalues)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        for x in range(len(self.dvalues)):\n",
    "            self.dinputs[x] = self.dinputs[x] - target[x]\n",
    "        self.dinputs = self.dinputs\n",
    "\n",
    "    def backwards1(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        self.y_true = y_true\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class Softmax_Cross():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.actvation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def backwards(self, dvalues, target):\n",
    "        self.target = target\n",
    "        self.dvalues = dvalues\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "\n",
    "# Klasa reprezentująca warstwę funkcji aktywacji ReLu - Rectified Linear Unit\n",
    "class ReLu6:\n",
    "    def forwards(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backwards(self, dvalues):\n",
    "        self.dvalues = dvalues\n",
    "        self.dinputs = self.dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "class LeakyReLu6:\n",
    "    def forwards(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.where(\n",
    "            self.inputs > 0, self.inputs, self.inputs * 0.001)\n",
    "\n",
    "    def backwards(self, dvalues):\n",
    "        self.dvalues = dvalues\n",
    "        self.dinputs = self.dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0.001  # 0.0001\n",
    "\n",
    "\n",
    "def flatten(inputs):\n",
    "    return inputs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa dropout do dezakatywowania losowych neuronówo zadany współczynnik procentowy\n",
    "class Dropout:\n",
    "\n",
    "    def __init__(self, drop_rate):\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forwards(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # jest 1 - drop_rate, bo w np.binomial okreslamy prawdopodobieństwo powodzenia. Czli 1 - 0.2 = 0.8\n",
    "        self.drop_arr = np.random.binomial(\n",
    "            1, 1 - self.drop_rate, size=self.inputs.shape) / (1 - self.drop_rate)\n",
    "        self.output = self.inputs * self.drop_arr\n",
    "\n",
    "    def backwards(self, dvalues):\n",
    "        self.dvalues = dvalues\n",
    "        self.dinputs = self.dvalues * self.drop_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learning_Rate_Optim:\n",
    "\n",
    "    def __init__(self, learning_rate, learning_rate_drop):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.new_learning_rate = learning_rate\n",
    "        self.learning_rate_drop = learning_rate_drop\n",
    "        self.iteration = 0\n",
    "\n",
    "    def learning_rate_change(self):\n",
    "        self.new_learning_rate = self.learning_rate * \\\n",
    "            (1 / (1 + self.learning_rate_drop * self.iteration))\n",
    "        self.iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('n2 nn3 weights 31014k', nn3.weights)\n",
    "np.save('n2 nn3 biases 31014k', nn3.biases)\n",
    "np.save('n2 cnn1 filters 31014k', cnn1.filters)\n",
    "np.save('n2 cnn1 biases 31014k', cnn1.biases)\n",
    "np.save('n2 acc_tab_train 31014k', acc_tab_train)\n",
    "np.save('n2 loss_tab_train 31014k', loss_tab_train)\n",
    "np.save('n2 acc_tab_test 31014k', acc_tab_test)\n",
    "np.save('n2 loss_tab_test 31014k', loss_tab_test)\n",
    "print(\"Zapisano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_filters_start = np.load('acnn5 nn3 cnn1_filters start.npy')\n",
    "nn_weights_start = np.load('acnn5 nn3 nn3_weights start.npy')\n",
    "print(\"Poprawnie wczytano dane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Moduł trenowania\"\"\"\n",
    "\"\"\"----------------------------------CNN8--------------------------------------------------\"\"\"\n",
    "\n",
    "\n",
    "for t in range(1):\n",
    "\n",
    "    cnn1 = ConvolutionalLayer(10, 5, 5, 1, 1, 1, \"True\")\n",
    "\n",
    "    pool1 = MaxPooling(2, 2)\n",
    "\n",
    "    relu1 = ReLu6()\n",
    "\n",
    "    leaky_relu1 = LeakyReLu6()\n",
    "\n",
    "    cnn2 = ConvolutionalLayer(5, 3, 3, 10, 1, 2, \"True\")\n",
    "\n",
    "    cnn2.filters = np.array([np.random.rand(\n",
    "        cnn2.filter_d, cnn2.filter_h, cnn2.filter_w) - 0.5 for x in range(cnn2.filters_num)])\n",
    "\n",
    "    cnn2.biases = cnn2.biases\n",
    "\n",
    "    pool2 = MaxPooling(2, 2)\n",
    "\n",
    "    relu2 = ReLu6()\n",
    "\n",
    "    leaky_relu2 = LeakyReLu6()\n",
    "\n",
    "    nn1 = NeuralNetwork6(245, 47, 2)\n",
    "\n",
    "    nn1_softmax = Softmax6()\n",
    "\n",
    "    loss = CrossEntropyLoss()\n",
    "\n",
    "    cnn1.filters = cnn1.filters * 0.1\n",
    "\n",
    "    cnn1.biases = cnn1.biases * 1\n",
    "\n",
    "    cnn2.filters = cnn2.filters * 0.1\n",
    "\n",
    "    cnn2.biases = cnn2.biases * 1\n",
    "\n",
    "    nn1.weights = nn1.weights * 0.00000000000000001\n",
    "\n",
    "    nn1.biases = nn1.biases * 1\n",
    "\n",
    "    lr = Learning_Rate_Optim(0.05, 0.05)  # dać po tym 0.005, 0.05\n",
    "\n",
    "    dropout1 = Dropout(0.9)\n",
    "\n",
    "    \"\"\"----------------------------------TEST TRENINGU--------------------------------------------------\"\"\"\n",
    "    acc_tab_train = []\n",
    "    loss_tab_train = []\n",
    "    acc_tab_test = []\n",
    "    loss_tab_test = []\n",
    "    for r in range(225):\n",
    "        lr.learning_rate_change()\n",
    "        k = 0\n",
    "        loss_stats = [[] for x in range(47)]  # dla emnista\n",
    "        loss1_list = []\n",
    "        loss_stats2 = [[] for x in range(47)]  # dla emnista\n",
    "        loss2_list = []\n",
    "        train_batch_size = 11280\n",
    "        train_batch_size = 500\n",
    "        range_num = r*train_batch_size\n",
    "        acc = 0\n",
    "        acc_fin = 0\n",
    "        acc_fin_ult = 0\n",
    "        i = 0\n",
    "        y = 1\n",
    "        l = 0\n",
    "        l2 = 0\n",
    "        for x in range(range_num, range_num+train_batch_size):\n",
    "            nn_input = train_images[x]\n",
    "            nn_output = np.zeros(47)\n",
    "            nn_output[train_labels[x]] = 1\n",
    "\n",
    "            # CNN and NN forward propagation\n",
    "            cnn1.conv_process(nn_input)  # CNN\n",
    "            pool1.forwards(cnn1.conv_output)\n",
    "            leaky_relu1.forwards(pool1.pooling_output)\n",
    "            cnn2.conv_process(leaky_relu1.output)\n",
    "            pool2.forwards(cnn2.conv_output)\n",
    "            leaky_relu2.forwards(pool2.pooling_output)\n",
    "            nn_input_flatten = (leaky_relu2.output).flatten()  # CNN\n",
    "            nn1.forwards(nn_input_flatten)\n",
    "            nn1_softmax.forwards(nn1.output)\n",
    "            loss.forwards(nn1_softmax.exp_inputs_norm, nn_output)\n",
    "\n",
    "            # NN loss\n",
    "            print(\"Train iteration: \" + str(x) + \" | loss 1: \" + str(loss.y_pred) +\n",
    "                  \" - label: \" + str(train_labels[x]))  # !!!!!!!!!!!!!\n",
    "            loss_stats[train_labels[x]].append(loss.y_pred)\n",
    "            loss1_list.append(loss.y_pred)\n",
    "\n",
    "            # NN backprop\n",
    "            loss.backwards(nn1_softmax.exp_inputs_norm, nn_output)\n",
    "            nn1.backwards(loss.dinputs)\n",
    "            leaky_relu2.backwards(\n",
    "                (nn1.dinputs).reshape(pool2.pooling_output.shape))\n",
    "            pool2.backwards1(leaky_relu2.dinputs)\n",
    "            cnn2.backwards(pool2.dinput)\n",
    "            leaky_relu1.backwards(cnn2.conv_dvalues)\n",
    "            pool1.backwards1(leaky_relu1.dinputs)\n",
    "            cnn1.backwards(pool1.dinput)  # CNN\n",
    "\n",
    "            # Update weight, biases and change learning rate\n",
    "            nn1.update(lr.new_learning_rate*1)  # 1\n",
    "            cnn2.update(lr.new_learning_rate*0.1)  # 0.1\n",
    "            cnn1.update(lr.new_learning_rate*0.01)  # 0.001\n",
    "\n",
    "            print(lr.new_learning_rate)\n",
    "            print(\"\\ncnn1\", cnn1.filters[5][0][0])\n",
    "            print(\"cnn2\", cnn2.filters[3][0][0])\n",
    "            print(\"nn1\", nn1.weights[90][0])\n",
    "            print(\n",
    "                \"-----------------------------------------------------------------------------------\")\n",
    "\n",
    "            acc = 1 if np.argmax(\n",
    "                nn1_softmax.exp_inputs_norm) == train_labels[x] else 0\n",
    "\n",
    "            acc_fin += acc\n",
    "            l += loss.y_pred\n",
    "            acc_fin_ult += acc\n",
    "            l2 += loss.y_pred2\n",
    "\n",
    "        acc_tab_train.append(acc_fin/train_batch_size)\n",
    "        loss_tab_train.append(l/train_batch_size)\n",
    "\n",
    "        acc_fin = acc_fin/train_batch_size\n",
    "        print(\"Accuracy:\", acc_fin*100, \"% Loss:\", l /\n",
    "              train_batch_size, \"Loss:\", l2/train_batch_size)\n",
    "\n",
    "print(\"ACC FIN ULT\", acc_fin_ult/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_input = train_images[x]\n",
    "nn_output = np.zeros(47)\n",
    "nn_output[train_labels[x]] = 1\n",
    "print(train_labels[x])\n",
    "img1 = train_images[x]\n",
    "f_min, f_max = np.amin(img1), np.amax(img1)\n",
    "img1 = (img1 - f_min) / (f_max - f_min)\n",
    "plt.show(plt.imshow(img1, cmap='gray'))\n",
    "print(nn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(x):\n",
    "    np.save('_cnn9 nn3 weights['+str(x)+\"]k\", nn1.weights)\n",
    "    np.save('_cnn9 nn3 biases['+str(x)+\"]k\", nn1.biases)\n",
    "    np.save('_cnn9 cnn1 filters['+str(x)+\"]k\", cnn1.filters)\n",
    "    np.save('_cnn9 cnn1 biases['+str(x)+\"]k\", cnn1.biases)\n",
    "    np.save('_cnn9 cnn2 filters['+str(x)+\"]k\", cnn2.filters)\n",
    "    np.save('_cnn9 cnn2 biases['+str(x)+\"]k\", cnn2.biases)\n",
    "    np.save('_cnn9 acc_tab_train['+str(x)+\"]k\", acc_tab_train)\n",
    "    np.save('_cnn9 loss_tab_train['+str(x)+\"]k\", loss_tab_train)\n",
    "    np.save('_cnn9 acc_tab_test['+str(x)+\"]k\", acc_tab_test)\n",
    "    np.save('_cnn9 loss_tab_test['+str(x)+\"]k\", loss_tab_test)\n",
    "    np.save('_cnn9 loss_stats['+str(x)+\"]k\", loss_stats)\n",
    "    np.save('_cnn9 loss1_list['+str(x)+\"]k\", loss1_list)\n",
    "    np.save('_cnn9 loss_stats2['+str(x)+\"]k\", loss_stats2)\n",
    "    np.save('_cnn9 loss2_list['+str(x)+\"]k\", loss2_list)\n",
    "    print(\"Zapisano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(112500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_filters = cnn1.filters\n",
    "cnn1_biases = cnn1.biases\n",
    "nn3_weights = nn3.weights\n",
    "nn3_biases = nn3.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.new_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data2(x):\n",
    "    np.save('acnn5 nn3 cnn1_filters start', cnn1_filters)\n",
    "    np.save('acnn5 nn3 cnn1_biases start', cnn1_biases)\n",
    "    np.save('acnn5 nn3 nn3_weights start', nn3_weights)\n",
    "    np.save('acnn5 nn3 nn3_biases start', nn3_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.y_pred)\n",
    "print(loss.y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = np.load('n2 nn3 weights[112499]k.npy')\n",
    "n2 = np.load('an2 nn3 weights[0]k.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MODUŁ TESTOWANIA\"\"\"\n",
    "\"\"\"----------------------------------TEST Z POPRAWIONYM POOLINGIEM - CNN,NN,Pool,relu,--------------------------------------------------\"\"\"\n",
    "\n",
    "for t in range(1):\n",
    "\n",
    "    acc_tab_test = []\n",
    "    loss_tab_test = []\n",
    "\n",
    "    # ostateczny nn i cnn - najważniesze!\n",
    "    for r in range(1):\n",
    "\n",
    "        # ----------------------------------TEST DATASET--------------------------------------------------\n",
    "        #nn + cnn + relu + maxpool\n",
    "        loss_stats = [[] for x in range(47)]  # dla emnista\n",
    "        test_bach_size = 18800\n",
    "        range_num = 0\n",
    "        acc = 0\n",
    "        acc_fin = 0\n",
    "        acc_fin_ult = 0\n",
    "        i = 0\n",
    "        y = 1\n",
    "        l = 0\n",
    "\n",
    "        for x in range(0, range_num+test_bach_size):\n",
    "            nn_input = test_images[x]\n",
    "            nn_output = np.zeros(47)  # dla emnista\n",
    "            nn_output[test_labels[x]] = 1\n",
    "\n",
    "            cnn1.conv_process(nn_input)  # CNN\n",
    "            pool1.forwards(cnn1.conv_output)\n",
    "            relu1.forwards(pool1.pooling_output)\n",
    "            nn_input_flatten = (relu1.output).flatten()  # CNN\n",
    "            nn1.forwards(nn_input_flatten)\n",
    "            nn1_softmax.forwards(nn1.output)\n",
    "            loss.forwards(nn1_softmax.exp_inputs_norm, nn_output)\n",
    "\n",
    "            # NN loss\n",
    "            print(\"Test iteration: \" + str(x) + \" | loss: \" + str(loss.y_pred) +\n",
    "                  \" - label: \" + str(test_labels[x]))  # !!!!!!!!!!!!!\n",
    "            loss_stats[test_labels[x]].append(loss.y_pred)\n",
    "            print(\"Train iteration: \" + str(x) + \" | loss 2: \" + str(loss.y_pred2) +\n",
    "                  \" - label: \" + str(train_labels[x]))  # !!!!!!!!!!!!!\n",
    "\n",
    "            acc = 1 if np.argmax(\n",
    "                nn1_softmax.exp_inputs_norm) == test_labels[x] else 0\n",
    "            acc_fin += acc\n",
    "            l += loss.y_pred\n",
    "\n",
    "        acc_tab_test.append(acc_fin/test_bach_size)\n",
    "        loss_tab_test.append(l/test_bach_size)\n",
    "\n",
    "        acc_fin = acc_fin/test_bach_size\n",
    "\n",
    "        print(\"Accuracy:\", acc_fin*100, \"% Loss:\", l/test_bach_size)\n",
    "        print(\"-------------------------\")\n",
    "\n",
    "print(\"ACC FIN ULT\", acc_fin_ult/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_wyniki_filtry_startowe = ConvolutionalLayer(10, 3, 3, 1, 1, 1)\n",
    "cnn_wyniki = ConvolutionalLayer(10, 3, 3, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_wyniki_filtry_startowe.filters = cnn_filters_start\n",
    "cnn_wyniki.filters = cnn1.filters\n",
    "cnn_wyniki_filtry_startowe.conv_process(train_images[0])\n",
    "cnn_wyniki.conv_process(train_images[0])\n",
    "\n",
    "print(cnn_wyniki_filtry_startowe.conv_output.shape)\n",
    "print((cnn_wyniki_filtry_startowe.conv_output[0]).shape)\n",
    "for x in range(10):\n",
    "    print(x)\n",
    "    img1 = cnn_wyniki_filtry_startowe.conv_output[x]\n",
    "    f_min, f_max = img1.min(), img1.max()\n",
    "    img1 = (img1 - f_min) / (f_max - f_min)\n",
    "    plt.show(plt.imshow(img1, cmap='gray'))\n",
    "\n",
    "    img2 = cnn_wyniki.conv_output[x]\n",
    "    f_min, f_max = img2.min(), img2.max()\n",
    "    img2 = (img2 - f_min) / (f_max - f_min)\n",
    "    plt.show(plt.imshow(img2, cmap='gray'))\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(cnn1.filters.shape[0]):\n",
    "    print(\"\\nstart filter\")\n",
    "    img1 = cnn1.filters[x][0]\n",
    "    f_min, f_max = np.amin(img1), np.amax(img1)\n",
    "    img1 = (img1 - f_min) / (f_max - f_min)\n",
    "    plt.show(plt.imshow(img1, cmap='gray'))\n",
    "    print(\"end filters\")\n",
    "    img2 = cnn1.filters[x][0]\n",
    "    f_min, f_max = np.amin(img2), np.amax(img2)\n",
    "    img2 = (img2 - f_min) / (f_max - f_min)\n",
    "    plt.show(plt.imshow(img2, cmap='gray'))\n",
    "    print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(cnn1.filters.shape[0]):\n",
    "    print(\"\\ncnn1 filter\")\n",
    "    img1 = cnn1.filters[x][0]\n",
    "    f_min, f_max = np.amin(img1), np.amax(img1)\n",
    "    img1 = (img1 - f_min) / (f_max - f_min)\n",
    "    plt.show(plt.imshow(img1, cmap='gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(cnn2.filters.shape[0]):\n",
    "    print(\"\\ncnn1 filter\")\n",
    "    img1 = cnn2.filters[x][0]\n",
    "    f_min, f_max = np.amin(img1), np.amax(img1)\n",
    "    img1 = (img1 - f_min) / (f_max - f_min)\n",
    "    plt.show(plt.imshow(img1, cmap='gray'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
